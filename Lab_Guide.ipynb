{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3u85Bos_JyP"
      },
      "source": [
        "# to download the dataset and pretrained model\n",
        "!gdown --id 1D79dNcLXu6mV1Uueo7EJJXtQm2AO9yUb\n",
        "# to unzip the file\n",
        "!unzip dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8txtr04PddQ"
      },
      "source": [
        "**Caption Testing on pre-trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Fd4amzPl76"
      },
      "source": [
        "# Caption testing on pre-trained model\n",
        "\n",
        "from pickle import load\n",
        "from numpy import argmax\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras_preprocessing.image import load_img\n",
        "from keras_preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "\n",
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "\n",
        "# extract features from the photo\n",
        "def extract_features(filename):\n",
        "  # load the model\n",
        "  model = VGG16()\n",
        "  # re-structure the model\n",
        "  model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "  # load the photo\n",
        "  image = load_img(filename, target_size=(224,224))\n",
        "  # convert the image pixels to numpy array\n",
        "  image = img_to_array(image)\n",
        "  # reshape data for the model\n",
        "  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "  # prepare the image for the VGG model\n",
        "  image = preprocess_input(image)\n",
        "  # get features\n",
        "  feature = model.predict(image, verbose=0)\n",
        "  return feature\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == integer:\n",
        "          return word\n",
        "  return None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "  # seed the generation process\n",
        "  in_text = 'startseq'\n",
        "  # iterate over the whole length of the sequence\n",
        "  for i in range(max_length):\n",
        "      # integer encode input sequence\n",
        "      sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "      # pad input\n",
        "      sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "      # predict next word\n",
        "      yhat = model.predict([photo, sequence], verbose=0)\n",
        "      # convert probability to integer\n",
        "      yhat = argmax(yhat)\n",
        "      # map integer to word\n",
        "      word = word_for_id(yhat, tokenizer)\n",
        "      # stop if cannot map the word\n",
        "      if word is None:\n",
        "          break\n",
        "      # append as input for generating the next word\n",
        "      in_text += ' ' + word\n",
        "      # stop if the end of the sequence is predicted\n",
        "      if word == 'endseq':\n",
        "          break\n",
        "  return in_text\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('/content/tokenizer.pkl', 'rb'))\n",
        "# pre-define the max sequence length (from pre-training)\n",
        "max_length = 34\n",
        "# load the pre-trained model\n",
        "model = load_model('/content/model_4.h5')\n",
        "# load and prepare the photograph\n",
        "photo = extract_features('/content/example.jpg')\n",
        "# display the image\n",
        "display.display(Image.open('/content/example.jpg'))\n",
        "# generate description\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(description)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyslKmMkPxJc"
      },
      "source": [
        "**Training using smaller dataset and pre-trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PabW0EQcKIP9"
      },
      "source": [
        "# Feature extraction on training images\n",
        "\n",
        "from os import listdir\n",
        "from pickle import dump\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras_preprocessing.image import load_img\n",
        "from keras_preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "  doc = load_doc(filename)\n",
        "  dataset = list()\n",
        "  # process line by line\n",
        "  for line in doc.split('\\n'):\n",
        "      # skip empty lines\n",
        "      if len(line) < 1:\n",
        "          continue\n",
        "      # get the image identifier\n",
        "      identifier = line.split('.')[0]\n",
        "      dataset.append(identifier)\n",
        "  return set(dataset)\n",
        "\n",
        "# extract features from each photo in the directory\n",
        "def extract_features(directory, dataset):\n",
        "  # load the model\n",
        "  model = VGG16()\n",
        "  # re-structure the model\n",
        "  model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "  # summarize\n",
        "  print(model.summary())\n",
        "  # extract features from each photo\n",
        "  features = dict()\n",
        "  for name in dataset:\n",
        "    # load an image from file\n",
        "    filename = directory + '/' + name + '.jpg'\n",
        "    image = load_img(filename, target_size=(224,224))\n",
        "    # convert the image pixels to a numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for the model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # prepare the image for the VGG model\n",
        "    image = preprocess_input(image)\n",
        "    # get features\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    # get image id\n",
        "    image_id = name.split('.')[0]\n",
        "    # store feature\n",
        "    features[image_id] = feature\n",
        "    print('>%s' % name)\n",
        "  return features\n",
        "\n",
        "# extract features from training images\n",
        "filename = '/content/Flickr8k_text/Flickr_100.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "train_features = extract_features('/content/Flickr8k_Dataset', train)\n",
        "print('Extracted Features of training data: %d' % len(train_features))\n",
        "# save to file\n",
        "dump(train_features, open('/content/features_train.pkl', 'wb'))\n",
        "\n",
        "# extract features from testing images\n",
        "filename = '/content/Flickr8k_text/Flickr_50.testImages.txt'\n",
        "test = load_set(filename)\n",
        "test_features = extract_features('/content/Flickr8k_Dataset', test)\n",
        "print('Extracted Features of test data: %d' % len(test_features))\n",
        "# save to file\n",
        "dump(test_features, open('/content/features_test.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDOyn4wOUFHE"
      },
      "source": [
        "# extract desccriptions for images\n",
        "\n",
        "import string\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "  mapping = dict()\n",
        "  # process lines\n",
        "  for line in doc.split('\\n'):\n",
        "      # split line by white space\n",
        "      tokens = line.split()\n",
        "      if len(line) < 2:\n",
        "          continue\n",
        "      # take the first token as the image id, the rest as the description\n",
        "      image_id, image_desc = tokens[0], tokens[1:]\n",
        "      # remove filename from image id\n",
        "      image_id = image_id.split('.')[0]\n",
        "      # convert description tokens back to string\n",
        "      image_desc = ' '.join(image_desc)\n",
        "      # create the list if needed:\n",
        "      if image_id not in mapping:\n",
        "          mapping[image_id] = list()\n",
        "      # store description\n",
        "      mapping[image_id].append(image_desc)\n",
        "  return mapping\n",
        "\n",
        "def clean_descriptions(descriptions):\n",
        "  # prepare translation table for removing punctuation\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  for key, desc_list in descriptions.items():\n",
        "      for i in range(len(desc_list)):\n",
        "          desc = desc_list[i]\n",
        "          # tokenize\n",
        "          desc = desc.split()\n",
        "          # convert to lower case\n",
        "          desc = [word.lower() for word in desc]\n",
        "          # remove punctuation from each token\n",
        "          desc = [w.translate(table) for w in desc]\n",
        "          # remove hanging 's' and 'a'\n",
        "          desc = [word for word in desc if len(word)>1]\n",
        "          # remove tokens with numbers\n",
        "          desc = [word for word in desc if word.isalpha()]\n",
        "          # store as string\n",
        "          desc_list[i] = ' '.join(desc)\n",
        "            \n",
        "# convert the loaded descriptions into a vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "  # build a list of all description strings\n",
        "  all_desc = set()\n",
        "  for key in descriptions.keys():\n",
        "      [all_desc.update(d.split()) for d in descriptions[key]]\n",
        "  return all_desc\n",
        "\n",
        "# save descriptions to file with one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "  lines = list()\n",
        "  for key, desc_list in descriptions.items():\n",
        "      for desc in desc_list:\n",
        "          lines.append(key + ' ' + desc)\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "    \n",
        "filename = '/content/Flickr8k_text/Flickr8k.token.txt'\n",
        "# load descriptions\n",
        "doc = load_doc(filename)\n",
        "# parse descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "# clean descriptions\n",
        "clean_descriptions(descriptions)\n",
        "# summarize vocabulary\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary size: %d' % len(vocabulary))\n",
        "# save to file\n",
        "save_descriptions(descriptions, '/content/descriptions.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjg4SLfyU1g0"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "from numpy import array\n",
        "from pickle import load\n",
        "from numpy import argmax\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras_preprocessing.image import load_img\n",
        "from keras_preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "  doc = load_doc(filename)\n",
        "  dataset = list()\n",
        "  # process line by line\n",
        "  for line in doc.split('\\n'):\n",
        "      # skip empty lines\n",
        "      if len(line) < 1:\n",
        "          continue\n",
        "      # get the image identifier\n",
        "      identifier = line.split('.')[0]\n",
        "      dataset.append(identifier)\n",
        "  return set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "  # load document\n",
        "  doc = load_doc(filename)\n",
        "  descriptions = dict()\n",
        "  for line in doc.split('\\n'):\n",
        "      # split line by white space\n",
        "      tokens = line.split()\n",
        "      # split id from description\n",
        "      image_id, image_desc = tokens[0], tokens[1:]\n",
        "      # skip images not in the set\n",
        "      if image_id in dataset:\n",
        "          # create list\n",
        "          if image_id not in descriptions:\n",
        "              descriptions[image_id] = list()\n",
        "          # wrap description in tokens\n",
        "          desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "          # store\n",
        "          descriptions[image_id].append(desc)\n",
        "  return descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "  # load all features\n",
        "  all_features = load(open(filename, 'rb'))\n",
        "  # filter features\n",
        "  features = {k: all_features[k] for k in dataset}\n",
        "  return features\n",
        "\n",
        "# create sequences of images, input sequences and output word for an image\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
        "  X1, X2, y = list(), list(), list()\n",
        "  # walk through each description for the image\n",
        "  for desc in desc_list:\n",
        "      # encode the sequence\n",
        "      seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "      # split one sequence into multiple X,y pairs\n",
        "      for i in range(1, len(seq)):\n",
        "          # split into input and output pair\n",
        "          in_seq, out_seq = seq[:i], seq[i]\n",
        "          # pad input sequence\n",
        "          in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "          # encode output sequence\n",
        "          out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "          # store\n",
        "          X1.append(photo)\n",
        "          X2.append(in_seq)\n",
        "          y.append(out_seq)\n",
        "  return array(X1), array(X2), array(y)\n",
        "\n",
        "# define captioning model\n",
        "def define_model():\n",
        "  model = load_model('/content/model_4.h5')\n",
        "  # summarize model\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "# intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
        "  # loop forever over images\n",
        "  while 1:\n",
        "      for key, desc_list in descriptions.items():\n",
        "          # retrieve the photo feature\n",
        "          photo = photos[key][0]\n",
        "          in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
        "          yield [in_img, in_seq], out_word\n",
        "\n",
        "# define train dataset\n",
        "\n",
        "# load training dataset (100)\n",
        "filename = '/content/Flickr8k_text/Flickr_100.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('/content/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "train_features = load_photo_features('/content/features_train.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('/content/tokenizer.pkl', 'rb'))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# pre-define the max sequence length (from pre-training)\n",
        "max_length = 34\n",
        "print('Description Length: %d' % max_length)\n",
        "\n",
        "# define the model\n",
        "model = define_model()\n",
        "# train the model, run epochs and save model after each epoch\n",
        "epochs = 5\n",
        "steps = len(train_descriptions)\n",
        "for i in range(epochs):\n",
        "  # create data generator\n",
        "  generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
        "  # fit for one epoch\n",
        "  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "  # save model\n",
        "  model.save('/content/lab_model_' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr8qepP8gVrK"
      },
      "source": [
        "# evaluating the trained model\n",
        "\n",
        "from numpy import argmax\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "  doc = load_doc(filename)\n",
        "  dataset = list()\n",
        "  # process line by line\n",
        "  for line in doc.split('\\n'):\n",
        "      # skip empty lines\n",
        "      if len(line) < 1:\n",
        "          continue\n",
        "      # get the image identifier\n",
        "      identifier = line.split('.')[0]\n",
        "      dataset.append(identifier)\n",
        "  return set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "  # load document\n",
        "  doc = load_doc(filename)\n",
        "  descriptions = dict()\n",
        "  for line in doc.split('\\n'):\n",
        "      # split line by white space\n",
        "      tokens = line.split()\n",
        "      # split id from description\n",
        "      image_id, image_desc = tokens[0], tokens[1:]\n",
        "      # skip images not in the set\n",
        "      if image_id in dataset:\n",
        "          # create list\n",
        "          if image_id not in descriptions:\n",
        "              descriptions[image_id] = list()\n",
        "          # wrap description in tokens\n",
        "          desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "          # store\n",
        "          descriptions[image_id].append(desc)\n",
        "  return descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "  # load all features\n",
        "  all_features = load(open(filename, 'rb'))\n",
        "  # filter features\n",
        "  features = {k: all_features[k] for k in dataset}\n",
        "  return features\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == integer:\n",
        "          return word\n",
        "  return None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "  # seed the generation process\n",
        "  in_text = 'startseq'\n",
        "  # iterate over the whole length of the sequence\n",
        "  for i in range(max_length):\n",
        "      # integer encode input sequence\n",
        "      sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "      # pad input\n",
        "      sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "      # predict next word\n",
        "      yhat = model.predict([photo, sequence], verbose=0)\n",
        "      # convert probability to integer\n",
        "      yhat = argmax(yhat)\n",
        "      # map integer to word\n",
        "      word = word_for_id(yhat, tokenizer)\n",
        "      # stop if cannot map the word\n",
        "      if word is None:\n",
        "          break\n",
        "      # append as input for generating the next word\n",
        "      in_text += ' ' + word\n",
        "      # stop if the end of the sequence is predicted\n",
        "      if word == 'endseq':\n",
        "          break\n",
        "  return in_text\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "  actual, predicted = list(), list()\n",
        "  # step over the whole set\n",
        "  for key, desc_list in descriptions.items():\n",
        "      # generate description\n",
        "      yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "      # store actual and predicted\n",
        "      references = [d.split() for d in desc_list]\n",
        "      actual.append(references)\n",
        "      predicted.append(yhat.split())\n",
        "  # calculate BLEU score\n",
        "  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "  \n",
        "  # display the predicted caption of the last photo\n",
        "  display.display(Image.open('/content/Flickr8k_Dataset/' + key +'.jpg'))\n",
        "  print(yhat)\n",
        "    \n",
        "# prepare test set\n",
        "\n",
        "# load test set\n",
        "filename = '/content/Flickr8k_text/Flickr_50.testImages.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions('/content/descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features('/content/features_test.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('/content/tokenizer.pkl', 'rb'))\n",
        "# pre-define the max sequence length (from pre-training)\n",
        "max_length = 34\n",
        "\n",
        "# load the model\n",
        "filename = '/content/lab_model_4.h5'\n",
        "model = load_model(filename)\n",
        "# evaluate model\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}